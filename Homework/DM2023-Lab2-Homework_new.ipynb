{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 戴天吟\n",
    "\n",
    "Student ID: 111034551\n",
    "\n",
    "GitHub ID: Charlotte038\n",
    "\n",
    "Kaggle name: Charlotte\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "[Snapshot1](w2v_xgb.png)\n",
    "[Snapshot2](Kaggle_Leaderboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the DM2023-Lab2-master. You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/09b1d0f3f8584d06848252277cb535f2) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/09b1d0f3f8584d06848252277cb535f2. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 27th 11:59 pm, Wednesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 31th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Home Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:16:28.859705Z",
     "start_time": "2023-12-23T05:16:28.747624Z"
    }
   },
   "outputs": [],
   "source": [
    "### Begin Assignment Here\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "### training data\n",
    "anger_train = pd.read_csv(\"C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/data/semeval/train/anger-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_train = pd.read_csv(\"C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-master/data/semeval/train/sadness-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_train = pd.read_csv(\"C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-master/data/semeval/train/fear-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_train = pd.read_csv(\"C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-master/data/semeval/train/joy-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "# combine 4 sub-dataset\n",
    "train_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)\n",
    "### testing data\n",
    "anger_test = pd.read_csv(\"C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_test = pd.read_csv(\"C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_test = pd.read_csv(\"C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_test = pd.read_csv(\"C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "\n",
    "# combine 4 sub-dataset\n",
    "test_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)\n",
    "# shuffle dataset\n",
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)\n",
    "print(\"Shape of Training df: \", train_df.shape)\n",
    "print(\"Shape of Testing df: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ** Exercise 1 (Take home): **  \n",
    "Plot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:19:42.361789Z",
     "start_time": "2023-12-23T05:19:35.861015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have loaded your train and test data into train_data and test_data respectively\n",
    "\n",
    "# Tokenize and count words in train data\n",
    "train_words = word_tokenize(' '.join(train_df))\n",
    "train_word_freq = Counter(train_words)\n",
    "train_top30 = train_word_freq.most_common(30)\n",
    "\n",
    "# Tokenize and count words in test data\n",
    "test_words = word_tokenize(' '.join(test_df))\n",
    "test_word_freq = Counter(test_words)\n",
    "test_top30 = test_word_freq.most_common(30)\n",
    "\n",
    "def plot_word_frequency(top_words, title):\n",
    "    words, counts = zip(*top_words)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(words)), counts, align='center')\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot top 30 words in train data\n",
    "plot_word_frequency(train_top30, 'Top 30 Words in Train Data')\n",
    "\n",
    "# Plot top 30 words in test data\n",
    "plot_word_frequency(test_top30, 'Top 30 Words in Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:33:48.091847Z",
     "start_time": "2023-12-23T05:33:44.529455Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize CountVectorizer and fit on train data\n",
    "BOW_vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "BOW_vectorizer.fit(train_df['text'])\n",
    "\n",
    "# Transform train and test data to Bag-of-Words features\n",
    "train_data_BOW_features = BOW_vectorizer.transform(train_df['text'])\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = BOW_vectorizer.get_feature_names()\n",
    "\n",
    "# Sum up word occurrences in train and test data\n",
    "train_term_count = np.asarray(train_data_BOW_features.sum(axis=0)).ravel()\n",
    "test_term_count = np.asarray(test_data_BOW_features.sum(axis=0)).ravel()\n",
    "\n",
    "# Create DataFrames for top 30 words and their frequencies in train and test data\n",
    "df_train = pd.DataFrame({'count': train_term_count, 'terms': feature_names})\n",
    "df_train = df_train.sort_values('count', ascending=False)[:30]\n",
    "\n",
    "df_test = pd.DataFrame({'count': test_term_count, 'terms': feature_names})\n",
    "df_test = df_test.sort_values('count', ascending=False)[:30]\n",
    "\n",
    "# Plotting top 30 word frequencies in Train and Test data\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 13))\n",
    "axs[0].set_title('Top 30 word frequency in Train data')\n",
    "axs[1].set_title('Top 30 word frequency in Test data')\n",
    "\n",
    "# Using seaborn barplot for visualization\n",
    "sns.barplot(x='terms', y='count', data=df_train, ax=axs[0])\n",
    "sns.barplot(x='terms', y='count', data=df_test, ax=axs[1])\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "for ax in axs:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ** Exercise 2 (Take home): **  \n",
    "Generate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:20:16.554285Z",
     "start_time": "2023-12-23T05:20:16.400772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display feature names for features 100 to 110\n",
    "print(\"Feature names for features 100 to 110:\")\n",
    "print(feature_names[100:111])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ** Exercise 3 (Take home): **  \n",
    "Can you interpret the results above? What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:20:35.547852Z",
     "start_time": "2023-12-23T05:20:35.526772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "'''\n",
    "# 1.Accuracy\n",
    "training accuracy: 0.99\n",
    "testing accuracy: 0.68\n",
    "-> training accuracy very high but testing accuracy is low -> overfitting \n",
    "\n",
    "# 2.precision, recall, f1-score\n",
    "Precision (精確率)：Precision表示模型在預測某個類別時的準確性，即在所有被分類為某一類別的樣本中，真正屬於該類別的比例。\n",
    "-> 高精確率表示模型將正確地識別出更多相應的類別\n",
    "Recall (召回率)：Recall表示模型成功識別出特定類別樣本的能力，即在所有屬於某一類別的樣本中，成功被模型分類為該類別的比例。\n",
    "-> 高召回率表示模型能夠更好地捕捉到該類別的所有實際樣本\n",
    "F1-score：F1-score是Precision和Recall的調和平均數，它綜合考慮了Precision和Recall，通常用於綜合評估模型的表現。\n",
    "-> F1-score越高，表示模型在Precision和Recall之間取得了良好的平衡\n",
    "-> 從結果來看，'joy'具有 high precision, high recall and high F1-score，表現最佳\n",
    "\n",
    "# 3.check by confusion matrix\n",
    "混淆矩陣的對角線(從左上到右下的元素)表示模型成功預測的數量(True Positive)，而非對角線的元素表示模型預測錯誤的情況(False Positive和False Negative)\n",
    "->\n",
    "第一行代表猜測為'anger'的結果。有57個樣本實際結果為'anger'，表示模型將這57個樣本正確地預測(True Positive)\n",
    "第二行代表猜測為'fear'的結果。有76個True Positive，其餘為False Positive和False Negative\n",
    "第三行代表猜測為'joy'的結果。有56個True Positive，其餘為False Positive和False Negative\n",
    "第四行代表猜測為'sadness'的結果。有47個True Positive，其餘為False Positive和False Negative\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ** Exercise 4 (Take home): **  \n",
    "Build a model using a ```Naive Bayes``` model and train it. What are the testing results? \n",
    "\n",
    "*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:20:53.231703Z",
     "start_time": "2023-12-23T05:20:45.667718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset (example with 20 Newsgroups dataset)\n",
    "data_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "data_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# Vectorize the text data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train = tfidf_vectorizer.fit_transform(data_train.data)\n",
    "X_test = tfidf_vectorizer.transform(data_test.data)\n",
    "\n",
    "# Initialize and train the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, data_train.target)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(data_test.target, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ** Exercise 5 (Take home): **  \n",
    "\n",
    "How do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:21:02.021553Z",
     "start_time": "2023-12-23T05:21:01.996716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# pros & cons, which situation use Naive Bayes model is better? And which situation use Decision Tree model is better?\n",
    "\n",
    "'''\n",
    "Naive Bayes vs. Decision Tree:\n",
    "\n",
    "Performance Metrics:\n",
    "NB: Naive Bayes classifiers work well on text data, especially in natural language processing tasks like sentiment analysis or document classification. They tend to be fast and require relatively small amounts of training data. However, they assume feature independence which might not hold true in some cases.\n",
    "DT: Decision Trees are versatile and can handle a variety of data types, providing interpretable rules. They can capture non-linear relationships between features and the target variable. However, they can easily overfit and may not generalize well if the tree becomes too deep.\n",
    "\n",
    "Interpretability:\n",
    "NB: Naive Bayes models are relatively simple and offer high interpretability. The probabilities they calculate for each class can be interpreted as likelihoods given the observed features.\n",
    "DT: Decision Trees provide explicit rules for classification, making them easy to interpret and visualize. Each split in the tree represents a decision based on a specific feature.\n",
    "\n",
    "Dataset Characteristics:\n",
    "NB: Naive Bayes assumes feature independence, which might not be valid for all datasets. If the features are correlated, it might affect the model's performance.\n",
    "DT: Decision Trees can handle complex relationships between features, but they can overfit if the tree is too deep. They might struggle with datasets where the decision boundaries are not aligned with feature axes.\n",
    "\n",
    "Interpretation based on Theoretical Background:\n",
    "Naive Bayes: The NB model assumes that features are conditionally independent given the class. This assumption might hold well for text classification tasks (e.g., spam detection), where words might be somewhat independent of each other given the class label (though this assumption is often violated to some extent).\n",
    "Decision Trees: DTs partition the feature space based on feature values, making decisions based on splitting criteria. They can capture complex relationships but might create overly complex trees that don't generalize well.\n",
    "\n",
    "Comparing Results:\n",
    "Performance: The NB model might perform well on text data due to its simplicity and the nature of text features, while Decision Trees might perform better if the relationships between features are more complex or non-linear.\n",
    "Interpretability: Both models offer interpretability, but Decision Trees provide explicit rules, which can be more easily understood by humans.\n",
    "\n",
    "The choice between NB and DT models depends on the nature of the dataset, the complexity of relationships between features, interpretability requirements, and the trade-off between accuracy and model complexity. Conducting a detailed analysis considering these aspects would provide deeper insights into the differences observed in their performances.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ** Exercise 6 (Take home): **  \n",
    "\n",
    "Plot the Training and Validation Accuracy and Loss (different plots), just like the images below.(Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n",
    "<table><tr>\n",
    "    <td><img src=\"pics/pic3.png\" style=\"width: 300px;\"/> </td>\n",
    "    <td><img src=\"pics/pic4.png\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:23:36.322956Z",
     "start_time": "2023-12-23T05:23:35.933974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/logs/training_log.csv\")\n",
    "training_log\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_log['accuracy'])\n",
    "plt.plot(training_log['val_accuracy'])\n",
    "plt.title('Training Accuracy per epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train accuracy', 'Validation accuracy'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_log['loss'])\n",
    "plt.plot(training_log['val_loss'])\n",
    "plt.title('Training Loss per epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train loss', 'Validation loss'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:23:50.333377Z",
     "start_time": "2023-12-23T05:23:50.320374Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Overfitting:\n",
    "Training vs. Validation Accuracy: Initially, both accuracies rise. However, if the training accuracy continues to increase while validation accuracy plateaus or decreases, it suggests overfitting. The model is memorizing the training data but fails to generalize to new data.\n",
    "Training vs. Validation Loss: A divergence between the training loss decreasing while the validation loss increases or remains stagnant indicates overfitting. The model learns the noise in the training data instead of the underlying patterns.\n",
    "\n",
    "Underfitting:\n",
    "Low Training and Validation Accuracy: Both accuracies remain low, suggesting the model is too simple to capture patterns in the data.\n",
    "High Training and Validation Loss: Elevated loss values across both training and validation datasets signify underfitting. The model struggles to learn relevant patterns.\n",
    "\n",
    "Relation to Overfitting/Underfitting:\n",
    "Overfitting: When the training performance (accuracy/loss) is significantly better than the validation performance, it suggests the model has memorized the training data too well, resulting in poor generalization.\n",
    "Underfitting: Occurs when both training and validation performances are subpar, indicating the model's inability to capture patterns in the data, possibly due to simplicity or poor training.\n",
    "\n",
    "-> this case is overfitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ** Exercise 7 (Take home): **  \n",
    "\n",
    "Now, we have the word vectors, but our input data is a sequence of words (or say sentence). \n",
    "How can we utilize these \"word\" vectors to represent the sentence data and train our model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:30:18.422139Z",
     "start_time": "2023-12-23T05:29:25.121022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# 1. Averaging Word Vectors:\n",
    "# Method: Calculate the average of word vectors for all words in the sentence.\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = \"GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin\"\n",
    "w2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "def average_word_vectors(words, model, num_features):\n",
    "    word_vecs = []\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            word_vecs.append(model[word])\n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(num_features)\n",
    "    word_vecs = np.array(word_vecs)\n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "# Example usage\n",
    "sentence = \"This is an example sentence\"\n",
    "words = sentence.split()\n",
    "sentence_vector = average_word_vectors(words, w2v_google_model, 300)  # Assuming 300-dimensional word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:40:26.095813Z",
     "start_time": "2023-12-23T05:40:26.066840Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. TF-IDF Weighted Word Vectors:\n",
    "# Method: Compute TF-IDF (Term Frequency-Inverse Document Frequency) weighted average of word vectors.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 假設你有一個包含句子的列表 sentences\n",
    "sentences = [\"This is an example sentence.\", \"Another example sentence.\"]\n",
    "\n",
    "# 初始化 TF-IDF 向量化器\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 將文本轉換為 TF-IDF 特徵\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "# 獲取詞彙表和詞彙到索引的映射\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "# 獲取 TF-IDF 加權詞向量\n",
    "def get_tfidf_weighted_vectors(sentence, model, tfidf_matrix, vocab):\n",
    "    words = sentence.split()\n",
    "    word_vecs = []\n",
    "    weights = []\n",
    "    for word in words:\n",
    "        if word in model and word in vocab:\n",
    "            word_vec = model[word]\n",
    "            word_idx = vocab[word]\n",
    "            tfidf_weight = tfidf_matrix[0, word_idx]  # Assuming only one document in tfidf_matrix\n",
    "            word_vecs.append(word_vec * tfidf_weight)\n",
    "            weights.append(tfidf_weight)\n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    weighted_avg = np.average(word_vecs, axis=0, weights=weights)\n",
    "    return weighted_avg\n",
    "\n",
    "# 獲取 TF-IDF 加權句子向量\n",
    "sentence = \"This is an example sentence.\"\n",
    "tfidf_weighted_vector = get_tfidf_weighted_vectors(sentence, w2v_google_model, tfidf_matrix, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:40:36.377372Z",
     "start_time": "2023-12-23T05:40:35.831338Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Doc2Vec or Paragraph Vectors:\n",
    "# Method: Train models like Doc2Vec or paragraph vectors to directly learn fixed-size representations for sentences.\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# 假設你有一個包含句子的列表 sentences\n",
    "tagged_data = [TaggedDocument(words=sentence.split(), tags=[str(i)]) for i, sentence in enumerate(sentences)]\n",
    "\n",
    "# 初始化 Doc2Vec 模型\n",
    "doc2vec_model = Doc2Vec(vector_size=300, window=5, min_count=1, workers=4, epochs=100)\n",
    "\n",
    "# 建立詞彙表並訓練模型\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "# 獲取句子向量\n",
    "sentence_vector = doc2vec_model.infer_vector(\"This is an example sentence.\".split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ** Exercise 8 (Take home): **  \n",
    "\n",
    "Generate a t-SNE visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-23T05:41:15.577720Z",
     "start_time": "2023-12-23T05:41:06.640686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Words of interest\n",
    "target_words = [\"angry\", \"happy\", \"sad\", \"fear\"]\n",
    "\n",
    "# Number of similar words to consider for each target word\n",
    "num_similar_words = 15\n",
    "\n",
    "# Initialize lists to store similar words and their vectors\n",
    "similar_words = []\n",
    "word_vectors = []\n",
    "\n",
    "# Retrieve similar words and their vectors for each target word\n",
    "for word in target_words:\n",
    "    # Get similar words and their similarity scores\n",
    "    similar = w2v_google_model.most_similar(word, topn=num_similar_words)\n",
    "    for item in similar:\n",
    "        similar_words.append(item[0])\n",
    "        word_vectors.append(w2v_google_model[item[0]])\n",
    "\n",
    "# t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=42)\n",
    "\n",
    "# Transform word vectors using t-SNE\n",
    "X_tsne = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the t-SNE visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], marker='o', c='blue')\n",
    "\n",
    "# Annotate each point with its respective word\n",
    "for i, word in enumerate(similar_words):\n",
    "    plt.annotate(word, xy=(X_tsne[i, 0], X_tsne[i, 1]), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom', fontsize=8)\n",
    "\n",
    "plt.title('t-SNE Visualization of Similar Words')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation steps of my Kaggle competition project are as follows:\n",
    "1. Load Data\n",
    "- This part of code performs three essential data processing steps: data loading, data merging, and data splitting. \n",
    "- Initially, I retrieve data from distinct source files, such as JSON and CSV files containing information about tweets, emotion labels, and data identification. \n",
    "- Next, I merge these datasets based on a common feature, like tweet IDs, creating a more comprehensive dataset to facilitate deeper insights into relationships within the data. \n",
    "- Finally, I divide the integrated dataset into training and testing sets based on identification labels, enabling the subsequent use of this data for training and evaluating machine learning models.\n",
    "\n",
    "\n",
    "2. Save Data\n",
    "- I save the data in Pickle format. \n",
    "- The pickle module implements binary protocols for serializing and de-serializing a Python object structure.\n",
    "\n",
    "\n",
    "3. Data Preprocessing\n",
    "- This code segment focuses on text preprocessing for Natural Language Processing (NLP) tasks.\n",
    "- The focus here is to prepare text data by removing noise (special characters, punctuation, stopwords) and standardizing it (lowercasing) for better analysis and model training in NLP tasks. \n",
    "- This preparation helps in extracting meaningful patterns from the text data.\n",
    "\n",
    "\n",
    "4. Other Feature Engineering\n",
    "- BoW + TF-IDF\n",
    "    - The teacher mentioned during class that Bog+TF-IDF might have good results.\n",
    "    - The significance lies in converting raw text data into numerical format (vectors) suitable for machine learning models.\n",
    "    - CountVectorizer creates a numerical representation of text, while TF-IDF further refines it by considering the importance of words in each document relative to the entire dataset.\n",
    "- Word2Vec\n",
    "    - The focus here is on converting words in tweets into continuous numerical representations (word embeddings) using Word2Vec, enabling machine learning models to understand the semantic meaning and context of words within the text data. \n",
    "    - This transformation allows for the utilization of word embeddings in various downstream tasks such as sentiment analysis or classification.\n",
    "\n",
    "\n",
    "5. Model \n",
    "- Support Vector Machine (SVM) model\n",
    "    - I initially utilized the CPU for the execution, but the processing time was excessively prolonged in generating results. Considering this, I might opt to switch to Colab for more efficient performance.\n",
    "    - SVC is a popular choice for classification tasks, especially when dealing with binary and multi-class classification problems. Its ability to handle non-linear data through kernel functions and its effectiveness in high-dimensional spaces make it suitable for sentiment classification tasks where distinguishing between multiple classes (emotions, in this case) is required. The 'linear' kernel specified here indicates a linear separation of classes in the feature space.\n",
    "- Decision Tree\n",
    "    - Also the processing time was excessively prolonged in generating results.\n",
    "- Logistic Regression\n",
    "    - [Snapshot3](Logistic_Regression.png)\n",
    "    - Logistic Regression is a classic linear model commonly used for binary classification tasks. \n",
    "    - It's efficient, interpretable, and works well for problems with linearly separable classes. \n",
    "    - In sentiment analysis, it can be effective when dealing with binary (positive/negative) or multi-class sentiment classification tasks.\n",
    "- Random Forest\n",
    "    - [Snapshot4](Random_Forest.png)\n",
    "    - Random Forest is an ensemble learning method that builds multiple decision trees and merges their predictions to improve accuracy and reduce overfitting.\n",
    "- Word2Vec+XGBoost (Best Performance)\n",
    "    - [Snapshot5](w2v_xgb.png)\n",
    "    - XGBoost is an effective and popular gradient boosting algorithm known for its high performance in classification tasks, providing strong predictive power and handling non-linear relationships between features and targets effectively. When combined with word embeddings, it can yield robust results in sentiment analysis by leveraging the semantics embedded in the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:57:28.608273Z",
     "start_time": "2023-12-24T06:57:20.144311Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:58:40.464083Z",
     "start_time": "2023-12-24T06:57:38.604523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "tweets = pd.read_json('C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/Kaggle Competition/dm2023-isa5810-lab2-homework/tweets_DM.json', lines=True)\n",
    "emotion_labels = pd.read_csv('C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/Kaggle Competition/dm2023-isa5810-lab2-homework/emotion.csv')\n",
    "data_identification = pd.read_csv('C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/Kaggle Competition/dm2023-isa5810-lab2-homework/data_identification.csv')\n",
    "\n",
    "# tweets = pd.read_json('C:/Users/flyin/OneDrive/桌面/HW/112-1/資料探勘/DMLab2/Kaggle Competition/dm2023-isa5810-lab2-homework/tweets_DM.json', lines=True)\n",
    "# emotion_labels = pd.read_csv('C:/Users/flyin/OneDrive/桌面/HW/112-1/資料探勘/DMLab2/Kaggle Competition/dm2023-isa5810-lab2-homework/emotion.csv')\n",
    "# data_identification = pd.read_csv('C:/Users/flyin/OneDrive/桌面/HW/112-1/資料探勘/DMLab2/Kaggle Competition/dm2023-isa5810-lab2-homework/data_identification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:58:40.597031Z",
     "start_time": "2023-12-24T06:58:40.464083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>827</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['mixedfeeling', 'butim...</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>368</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x29d0...</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>498</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2a6a...</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>840</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x24fa...</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>360</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Sundayvibes'], 'tweet...</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score          _index  \\\n",
       "0           391  hashtag_tweets   \n",
       "1           433  hashtag_tweets   \n",
       "2           232  hashtag_tweets   \n",
       "3           376  hashtag_tweets   \n",
       "4           989  hashtag_tweets   \n",
       "...         ...             ...   \n",
       "1867530     827  hashtag_tweets   \n",
       "1867531     368  hashtag_tweets   \n",
       "1867532     498  hashtag_tweets   \n",
       "1867533     840  hashtag_tweets   \n",
       "1867534     360  hashtag_tweets   \n",
       "\n",
       "                                                   _source  \\\n",
       "0        {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1        {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2        {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3        {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4        {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "...                                                    ...   \n",
       "1867530  {'tweet': {'hashtags': ['mixedfeeling', 'butim...   \n",
       "1867531  {'tweet': {'hashtags': [], 'tweet_id': '0x29d0...   \n",
       "1867532  {'tweet': {'hashtags': [], 'tweet_id': '0x2a6a...   \n",
       "1867533  {'tweet': {'hashtags': [], 'tweet_id': '0x24fa...   \n",
       "1867534  {'tweet': {'hashtags': ['Sundayvibes'], 'tweet...   \n",
       "\n",
       "                  _crawldate   _type  \n",
       "0        2015-05-23 11:42:47  tweets  \n",
       "1        2016-01-28 04:52:09  tweets  \n",
       "2        2017-12-25 04:39:20  tweets  \n",
       "3        2016-01-24 23:53:05  tweets  \n",
       "4        2016-01-08 17:18:59  tweets  \n",
       "...                      ...     ...  \n",
       "1867530  2015-05-12 12:51:52  tweets  \n",
       "1867531  2017-10-02 17:54:04  tweets  \n",
       "1867532  2016-10-10 11:04:32  tweets  \n",
       "1867533  2016-09-02 14:25:06  tweets  \n",
       "1867534  2016-11-16 01:40:07  tweets  \n",
       "\n",
       "[1867535 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:59:24.461094Z",
     "start_time": "2023-12-24T06:58:40.602723Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Load the data from tweets_DM.json\n",
    "id_list = []\n",
    "text_list = []\n",
    "score_list = []\n",
    "\n",
    "with open('C:/Users/user/Desktop/碩士課程資料/112-1/資料探勘/HW/DM2023-Lab2-Master/Kaggle Competition/dm2023-isa5810-lab2-homework/tweets_DM.json', 'r') as f:\n",
    "    into_list = f.readlines()\n",
    "    for data_json in into_list:\n",
    "        data = json.loads(data_json)\n",
    "        score_list.append(data['_score'])\n",
    "        id_list.append(data['_source']['tweet']['tweet_id'])\n",
    "        text_list.append(data['_source']['tweet']['text'])\n",
    "\n",
    "data_tweet = {\"tweet_id\": id_list, \"text\": text_list, \"score\": score_list}\n",
    "tweets = pd.DataFrame(data_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:59:24.510271Z",
     "start_time": "2023-12-24T06:59:24.469458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  score\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...    391\n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...    433\n",
       "2        0x28b412  Confident of your obedience, I write to you, k...    232\n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>    376\n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...    989\n",
       "...           ...                                                ...    ...\n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...    827\n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...    368\n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...    498\n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...    840\n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>    360\n",
       "\n",
       "[1867535 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:59:24.568291Z",
     "start_time": "2023-12-24T06:59:24.518514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x38dba0</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x300ea2</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x360b99</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x22eecf</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x2fb282</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id       emotion\n",
       "0        0x3140b1       sadness\n",
       "1        0x368b73       disgust\n",
       "2        0x296183  anticipation\n",
       "3        0x2bd6e1           joy\n",
       "4        0x2ee1dd  anticipation\n",
       "...           ...           ...\n",
       "1455558  0x38dba0           joy\n",
       "1455559  0x300ea2           joy\n",
       "1455560  0x360b99          fear\n",
       "1455561  0x22eecf           joy\n",
       "1455562  0x2fb282  anticipation\n",
       "\n",
       "[1455563 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:59:24.608132Z",
     "start_time": "2023-12-24T06:59:24.573477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification\n",
       "0        0x28cc61           test\n",
       "1        0x29e452          train\n",
       "2        0x2b3819          train\n",
       "3        0x2db41f           test\n",
       "4        0x2a2acc          train\n",
       "...           ...            ...\n",
       "1867530  0x227e25          train\n",
       "1867531  0x293813          train\n",
       "1867532  0x1e1a7e          train\n",
       "1867533  0x2156a5          train\n",
       "1867534  0x2bb9d2          train\n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:59:30.636940Z",
     "start_time": "2023-12-24T06:59:24.612467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "merged_data = pd.merge(tweets, emotion_labels, on='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:59:42.354692Z",
     "start_time": "2023-12-24T06:59:30.636940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>391</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>433</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>232</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>376</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>989</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  score  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...    391   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...    433   \n",
       "2  0x28b412  Confident of your obedience, I write to you, k...    232   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>    376   \n",
       "4  0x2de201  \"Trust is not the same as faith. A friend is s...    989   \n",
       "\n",
       "  identification       emotion  \n",
       "0          train  anticipation  \n",
       "1          train       sadness  \n",
       "2           test           NaN  \n",
       "3          train          fear  \n",
       "4           test           NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge datasets\n",
    "merged_data=pd.merge(tweets,data_identification,on='tweet_id',how='outer')\n",
    "merged_data= pd.merge(merged_data,emotion_labels,on='tweet_id',how= 'outer')\n",
    "merged_data.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:59:43.858204Z",
     "start_time": "2023-12-24T06:59:42.354692Z"
    }
   },
   "outputs": [],
   "source": [
    "# split train and test data by its' identification\n",
    "\n",
    "df_train = merged_data[merged_data['identification']=='train']\n",
    "df_test = merged_data[merged_data['identification']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:59:43.922618Z",
     "start_time": "2023-12-24T06:59:43.866192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>391</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>433</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>376</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>120</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>1021</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867526</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>94</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867527</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>627</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867528</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>274</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>840</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>360</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  score  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...    391   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...    433   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>    376   \n",
       "5        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...    120   \n",
       "6        0x2c91a8       Still waiting on those supplies Liscus. <LH>   1021   \n",
       "...           ...                                                ...    ...   \n",
       "1867526  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...     94   \n",
       "1867527  0x38959e  In every circumtance I'd like to be thankful t...    627   \n",
       "1867528  0x2cbca6  there's currently two girls walking around the...    274   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...    840   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>    360   \n",
       "\n",
       "        identification       emotion  \n",
       "0                train  anticipation  \n",
       "1                train       sadness  \n",
       "3                train          fear  \n",
       "5                train           joy  \n",
       "6                train  anticipation  \n",
       "...                ...           ...  \n",
       "1867526          train           joy  \n",
       "1867527          train           joy  \n",
       "1867528          train           joy  \n",
       "1867533          train           joy  \n",
       "1867534          train           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T06:59:43.971622Z",
     "start_time": "2023-12-24T06:59:43.930770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>232</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>989</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>66</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>104</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>310</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>\"For this is the message that ye heard from th...</td>\n",
       "      <td>602</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>\"There is a lad here, which hath five barley l...</td>\n",
       "      <td>598</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>827</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>368</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>498</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  score  \\\n",
       "2        0x28b412  Confident of your obedience, I write to you, k...    232   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...    989   \n",
       "9        0x218443  When do you have enough ? When are you satisfi...     66   \n",
       "30       0x2939d5  God woke you up, now chase the day #GodsPlan #...    104   \n",
       "33       0x26289a  In these tough times, who do YOU turn to as yo...    310   \n",
       "...           ...                                                ...    ...   \n",
       "1867525  0x2913b4  \"For this is the message that ye heard from th...    602   \n",
       "1867529  0x2a980e  \"There is a lad here, which hath five barley l...    598   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...    827   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...    368   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...    498   \n",
       "\n",
       "        identification emotion  \n",
       "2                 test     NaN  \n",
       "4                 test     NaN  \n",
       "9                 test     NaN  \n",
       "30                test     NaN  \n",
       "33                test     NaN  \n",
       "...                ...     ...  \n",
       "1867525           test     NaN  \n",
       "1867529           test     NaN  \n",
       "1867530           test     NaN  \n",
       "1867531           test     NaN  \n",
       "1867532           test     NaN  \n",
       "\n",
       "[411972 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T07:00:04.820230Z",
     "start_time": "2023-12-24T07:00:00.850753Z"
    }
   },
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "df_train.to_pickle(\"df_train.pkl\") \n",
    "df_test.to_pickle(\"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T07:00:07.587706Z",
     "start_time": "2023-12-24T07:00:04.828284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the DataFrame from pickle file\n",
    "df_train = pd.read_pickle(\"df_train.pkl\")\n",
    "df_test = pd.read_pickle(\"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T07:00:11.293258Z",
     "start_time": "2023-12-24T07:00:10.794064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T07:34:56.976809Z",
     "start_time": "2023-12-24T07:00:15.701515Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  People who post \"add me on #Snapchat\" must be ...   \n",
      "1  @brianklaas As we see, Trump is dangerous to #...   \n",
      "3                Now ISSA is stalking Tasha 😂😂😂 <LH>   \n",
      "5  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
      "6       Still waiting on those supplies Liscus. <LH>   \n",
      "\n",
      "                                          clean_text       emotion  \n",
      "0  people post add snapchat must dehydrated cuz m...  anticipation  \n",
      "1  brianklaas see trump dangerous freepress aroun...       sadness  \n",
      "3                             issa stalking tasha lh          fear  \n",
      "5  riskshow thekevinallison thx best time tonight...           joy  \n",
      "6                   still waiting supplies liscus lh  anticipation  \n",
      "                                                 text  \\\n",
      "2   Confident of your obedience, I write to you, k...   \n",
      "4   \"Trust is not the same as faith. A friend is s...   \n",
      "9   When do you have enough ? When are you satisfi...   \n",
      "30  God woke you up, now chase the day #GodsPlan #...   \n",
      "33  In these tough times, who do YOU turn to as yo...   \n",
      "\n",
      "                                           clean_text emotion  \n",
      "2   confident obedience write knowing even ask phi...     NaN  \n",
      "4   trust faith friend someone trust putting faith...     NaN  \n",
      "9   enough satisfied goal really money materialism...     NaN  \n",
      "30            god woke chase day godsplan godswork lh     NaN  \n",
      "33                    tough times turn symbol hope lh     NaN  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    clean_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "# Apply preprocessing to 'text' column in the df_train DataFrame\n",
    "df_train['clean_text'] = df_train['text'].apply(preprocess_text)\n",
    "\n",
    "# Apply preprocessing to 'text' column in the df_test DataFrame\n",
    "df_test['clean_text'] = df_test['text'].apply(preprocess_text)\n",
    "\n",
    "# Displaying the updated DataFrame with the cleaned text for df_train\n",
    "print(df_train[['text', 'clean_text', 'emotion']].head())\n",
    "\n",
    "# Displaying the updated DataFrame with the cleaned text for df_test\n",
    "print(df_test[['text', 'clean_text', 'emotion']].head())  # Ensure 'emotion' is included for verification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BoW+TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T07:36:08.517846Z",
     "start_time": "2023-12-24T07:34:56.982722Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Create CountVectorizer for df_train\n",
    "count_vectorizer_train = CountVectorizer(max_features=1000)\n",
    "bow_matrix_train = count_vectorizer_train.fit_transform(df_train['text'])\n",
    "\n",
    "# Convert BoW to TF-IDF representation for df_train\n",
    "tfidf_transformer_train = TfidfTransformer()\n",
    "X_train = tfidf_transformer_train.fit_transform(bow_matrix_train)\n",
    "y_train = df_train['emotion']\n",
    "\n",
    "# Apply the same transformation to df_test using the fitted transformers from df_train\n",
    "bow_matrix_test = count_vectorizer_train.transform(df_test['text'])\n",
    "X_test = tfidf_transformer_train.transform(bow_matrix_test)\n",
    "y_test = df_test['emotion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T08:18:07.101468Z",
     "start_time": "2023-12-24T08:18:07.077462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1455563x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13350564 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T08:18:14.537734Z",
     "start_time": "2023-12-24T08:18:14.513726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          anticipation\n",
       "1               sadness\n",
       "3                  fear\n",
       "5                   joy\n",
       "6          anticipation\n",
       "               ...     \n",
       "1867526             joy\n",
       "1867527             joy\n",
       "1867528             joy\n",
       "1867533             joy\n",
       "1867534             joy\n",
       "Name: emotion, Length: 1455563, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T08:17:43.323648Z",
     "start_time": "2023-12-24T08:17:43.299545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<411972x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4672243 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T16:41:33.184425Z",
     "start_time": "2023-12-25T16:41:33.010377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2          NaN\n",
       "4          NaN\n",
       "9          NaN\n",
       "30         NaN\n",
       "33         NaN\n",
       "          ... \n",
       "1867525    NaN\n",
       "1867529    NaN\n",
       "1867530    NaN\n",
       "1867531    NaN\n",
       "1867532    NaN\n",
       "Name: emotion, Length: 411972, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T16:47:26.135380Z",
     "start_time": "2023-12-25T16:41:37.966711Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the text into words (assuming 'text' is the preprocessed text)\n",
    "tokenized_tweets_train = df_train['text'].apply(lambda x: x.split())\n",
    "tokenized_tweets_test = df_test['text'].apply(lambda x: x.split())\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_tweets_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get vector representation of a tweet\n",
    "def get_tweet_vector(tweet_tokens):\n",
    "    tweet_vector = np.zeros((word2vec_model.vector_size,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    for word in tweet_tokens:\n",
    "        if word in word2vec_model.wv:\n",
    "            tweet_vector = np.add(tweet_vector, word2vec_model.wv[word])\n",
    "            num_words += 1\n",
    "    if num_words != 0:\n",
    "        tweet_vector = np.divide(tweet_vector, num_words)\n",
    "    return tweet_vector\n",
    "\n",
    "# Create vectors for each tweet in train and test sets\n",
    "tweet_vectors_train = tokenized_tweets_train.apply(lambda x: get_tweet_vector(x))\n",
    "tweet_vectors_test = tokenized_tweets_test.apply(lambda x: get_tweet_vector(x))\n",
    "\n",
    "# Convert tweet vectors to arrays\n",
    "X_train = np.array(tweet_vectors_train.tolist())\n",
    "X_test = np.array(tweet_vectors_test.tolist())\n",
    "\n",
    "# Define target variable 'emotion'\n",
    "y_train = df_train['emotion']\n",
    "y_test = df_test['emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine (SVM) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-25T16:42:02.696Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Handling missing values in 'emotion' column for df_test\n",
    "df_test['emotion'].fillna('unknown', inplace=True)  # Filling NaNs with a default category like 'unknown'\n",
    "\n",
    "# Encode emotions to numerical labels for df_train\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(df_train['emotion'])\n",
    "\n",
    "# Define the classifier (SVM)\n",
    "svm_classifier = SVC(kernel='linear', C=1, probability=True)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test set (df_test)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Decode the predicted labels back to emotions\n",
    "predicted_emotions = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Create a DataFrame with 'tweet_id' and 'predicted_emotion'\n",
    "submission_df = pd.DataFrame({'tweet_id': df_test['tweet_id'], 'predicted_emotion': predicted_emotions})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission_SVC.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-23T11:57:42.771Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Choose a model and train - Decision Tree Classifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-23T11:57:44.584Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Using Logistic Regression model\n",
    "log_reg_classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Encode the target variable 'y_train'\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Training the model on the training data\n",
    "log_reg_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Performing predictions on the test set\n",
    "predicted_emotions = log_reg_classifier.predict(X_test)\n",
    "\n",
    "# Decoding the predicted labels\n",
    "predicted_emotions = label_encoder.inverse_transform(predicted_emotions)\n",
    "\n",
    "# Adding predicted emotions to the df_test DataFrame\n",
    "df_test['predicted_emotion'] = predicted_emotions\n",
    "\n",
    "# Creating the submission file 'submission.csv'\n",
    "submission_df = df_test[['tweet_id', 'predicted_emotion']]\n",
    "submission_df.to_csv('submission_Logistic Regression.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T06:22:37.096113Z",
     "start_time": "2023-12-29T06:22:31.939137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest Classifier\n",
    "\n",
    "# Using Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust parameters like n_estimators\n",
    "\n",
    "# Encode the target variable 'y_train'\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Training the model on the training data\n",
    "rf_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Performing predictions on the test set\n",
    "predicted_emotions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Decoding the predicted labels\n",
    "predicted_emotions = label_encoder.inverse_transform(predicted_emotions)\n",
    "\n",
    "# Adding predicted emotions to the df_test DataFrame\n",
    "df_test['predicted_emotion'] = predicted_emotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T00:34:46.120082Z",
     "start_time": "2023-12-25T00:34:45.443385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the submission file 'submission.csv'\n",
    "submission_df = df_test[['tweet_id', 'predicted_emotion']]\n",
    "submission_df.to_csv('submission_RandomForest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T00:42:01.429582Z",
     "start_time": "2023-12-25T00:42:00.388736Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_RF=df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T01:07:36.312669Z",
     "start_time": "2023-12-25T00:58:39.581691Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Tokenize the text into words (assuming 'clean_text' is the preprocessed text)\n",
    "tokenized_tweets_train = df_train['clean_text'].apply(lambda x: x.split())\n",
    "tokenized_tweets_test = df_test['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_tweets_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get vector representation of a tweet\n",
    "def get_tweet_vector(tweet_tokens):\n",
    "    tweet_vector = np.zeros((word2vec_model.vector_size,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    for word in tweet_tokens:\n",
    "        if word in word2vec_model.wv:\n",
    "            tweet_vector = np.add(tweet_vector, word2vec_model.wv[word])\n",
    "            num_words += 1\n",
    "    if num_words != 0:\n",
    "        tweet_vector = np.divide(tweet_vector, num_words)\n",
    "    return tweet_vector\n",
    "\n",
    "# Create vectors for each tweet in train and test sets\n",
    "tweet_vectors_train = tokenized_tweets_train.apply(lambda x: get_tweet_vector(x))\n",
    "tweet_vectors_test = tokenized_tweets_test.apply(lambda x: get_tweet_vector(x))\n",
    "\n",
    "# Convert tweet vectors to arrays\n",
    "X_train = np.array(tweet_vectors_train.tolist())\n",
    "X_test = np.array(tweet_vectors_test.tolist())\n",
    "\n",
    "# Extracting the target variable 'emotion' from df_train\n",
    "y_train = df_train['emotion']\n",
    "\n",
    "# Encoding the target variable 'y_train'\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Define a classifier (for example, XGBoost)\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Fit the classifier on the training data using tweet vectors\n",
    "xgb_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Decode the predicted labels back to emotions\n",
    "predicted_emotions = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Add predicted emotions to the df_test DataFrame\n",
    "df_test['predicted_emotion_w2v_xgb'] = predicted_emotions\n",
    "\n",
    "# Create the submission file 'submission_w2v_xgb.csv'\n",
    "submission_w2v_xgb_df = df_test[['tweet_id', 'predicted_emotion_w2v_xgb']]\n",
    "submission_w2v_xgb_df.to_csv('submission_w2v_xgb.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "371.333px",
    "left": "183px",
    "top": "110.229px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
